########################################################################
#...Prompt that generated this program:
"""
Interactive development session;
prompt still under development.
"""

########################################################################
#...Pseudocode for this program:
"""
# Parse command-line arguments
DEFINE parser AS ArgumentParser()
ADD argument parser for "disease_file" (TSV format)
ADD argument parser for "control_file" (TSV format)
PARSE arguments

# Load disease and control datasets
LOAD disease_file INTO disease_df (TSV format)
LOAD control_file INTO control_df (TSV format)

# Assign labels (1 for disease, 0 for control)
SET disease_df["label"] = 1
SET control_df["label"] = 0

# Combine both datasets
SET df = CONCATENATE(disease_df, control_df)

# Drop genome_name column (not needed)
REMOVE column "genome_name" from df

# Ensure correct data types
CONVERT df["sampleID"] TO STRING
CONVERT df["genome_id"] TO STRING
CONVERT df["score"] TO FLOAT
CONVERT df["num_roles"] TO INTEGER

# Function to fit power-law distribution and determine threshold
FUNCTION fit_power_law(scores):
    SORT scores in descending order
    DEFINE power_law(x, a, b) AS b * x^(-a)
    
    TRY:
        FIT curve using power_law function
        SET alpha = estimated exponent
        RETURN percentile threshold based on (1 - 1/alpha)
    EXCEPT:
        RETURN 90th percentile threshold as fallback

# Feature Extraction
SET features = EMPTY LIST
GROUP df BY "sampleID"

FOR EACH (sample, group) IN sample_groups:
    GET scores from group["score"]
    GET num_roles from group["num_roles"]

    # Determine adaptive threshold for significant genomes
    SET threshold = fit_power_law(scores)

    # Extract significant genomes
    SET significant = scores WHERE scores >= threshold
    SET high_confidence = scores WHERE num_roles >= 4

    # Compute feature values
    SET num_significant = COUNT(significant)
    SET mean_significant = MEAN(significant) IF num_significant > 0 ELSE 0
    SET num_high_conf = COUNT(high_confidence)
    SET mean_high_conf = MEAN(high_confidence) IF num_high_conf > 0 ELSE 0
    SET score_entropy = -SUM((scores / SUM(scores)) * LOG2(scores / SUM(scores)))

    # Store extracted features
    APPEND [sample, num_significant, mean_significant, num_high_conf, mean_high_conf, score_entropy] TO features

# Convert features to a structured dataset
SET feature_df = CREATE DATAFRAME(features, columns=["sampleID", "num_significant", "mean_significant", "num_high_conf", "mean_high_conf", "score_entropy"])

# Merge with sample labels
SET sample_labels = DISTINCT df[["sampleID", "label"]]
SET feature_df = MERGE feature_df WITH sample_labels ON "sampleID"

# Output extracted features as TSV to STDOUT
WRITE feature_df TO STDOUT AS TSV (TAB-SEPARATED FORMAT)
"""

########################################################################
#...Code generated by Grimoire:
import argparse
import pandas as pd
import numpy as np
import sys
from scipy.stats import gmean
from scipy.optimize import curve_fit

# Argument Parsing
parser = argparse.ArgumentParser(description="Feature Extraction for Disease & Control Genomes")
parser.add_argument("disease_file", type=str, help="Path to the disease dataset (TSV format)")
parser.add_argument("control_file", type=str, help="Path to the control dataset (TSV format)")
args = parser.parse_args()

# Load disease and control datasets
disease_df = pd.read_csv(args.disease_file, sep="\t")
control_df = pd.read_csv(args.control_file, sep="\t")

# Assign labels (1 for disease, 0 for control)
disease_df["label"] = 1
control_df["label"] = 0

# Combine both datasets
df = pd.concat([disease_df, control_df])

# Drop genome_name (not needed)
df = df.drop(columns=["genome_name"])

# Ensure correct data types
df["sampleID"] = df["sampleID"].astype(str)
df["genome_id"] = df["genome_id"].astype(str)
df["score"] = df["score"].astype(float)
df["num_roles"] = df["num_roles"].astype(int)

### Adaptive Threshold Selection ###
def fit_power_law(scores):
    """Fits a Zipf/Pareto distribution to genome scores and finds threshold."""
    def power_law(x, a, b):
        return b * x ** (-a)

    x = np.arange(1, len(scores) + 1)
    y = np.sort(scores)[::-1]  # Sort in descending order

    try:
        popt, _ = curve_fit(power_law, x, y, maxfev=10000)
        alpha = popt[0]  # Exponent of power law
        cutoff = np.nanpercentile(scores, 100 * (1 - 1/alpha))  # Tail cutoff
    except:
        cutoff = np.nanpercentile(scores, 90)  # Fallback threshold

    return cutoff

### Feature Extraction ###
features = []
sample_groups = df.groupby("sampleID")

for sample, group in sample_groups:
    scores = group["score"].values
    num_roles = group["num_roles"].values

    # Determine adaptive threshold
    threshold = fit_power_law(scores)

    # Extract significant genomes
    significant = scores[scores >= threshold]
    high_confidence = scores[num_roles >= 4]  # Confidence-based filtering

    # Compute feature values using geometric mean
    if len(significant) > 0:
        mean_significant = gmean(significant)
    else:
        mean_significant = 0

    if len(high_confidence) > 0:
        mean_high_conf = gmean(high_confidence)
    else:
        mean_high_conf = 0

    num_significant = len(significant)  # How many genomes exceed threshold?
    num_high_conf = len(high_confidence)  # Count of confident genomes
    score_entropy = -np.sum((scores / np.sum(scores)) * np.log2(scores / np.sum(scores)))  # Diversity metric

    # Store extracted features
    features.append([sample, num_significant, mean_significant, num_high_conf, mean_high_conf, score_entropy])

# Convert features to a structured dataset
feature_df = pd.DataFrame(features, columns=["sampleID", "num_significant", "mean_significant", "num_high_conf", "mean_high_conf", "score_entropy"])

# Merge with sample labels
sample_labels = df[["sampleID", "label"]].drop_duplicates()
feature_df = feature_df.merge(sample_labels, on="sampleID")

# Output as TSV to STDOUT
feature_df.to_csv(sys.stdout, sep="\t", index=False)
